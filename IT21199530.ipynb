{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload your Kaggle API token JSON file\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d paultimothymooney/breast-histopathology-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q breast-histopathology-images.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "# Preprocessing/Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Model creation\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Evaluatiom\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining directories\n",
    "\n",
    "base_path = \"dataset/IDC_regular_ps50_idx5/\"\n",
    "files = listdir(base_path)\n",
    "\n",
    "# Length of the samples is usually the no of patients\n",
    "\n",
    "print(\"Total no of patients: \"+ str(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data source into an array\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "  patient_id = files[i]\n",
    "  for c in [0,1]:\n",
    "    patientid_path = base_path + patient_id\n",
    "    class_path = patientid_path + \"/\" + str(c) + \"/\"\n",
    "    samples = listdir(class_path)\n",
    "    for picture in samples:\n",
    "      image_path = class_path + picture\n",
    "      dataset.append([image_path, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of images: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presentation of dataset 2d array\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the dataset due to perfomance concerns\n",
    "\n",
    "limit = len(dataset) / 8\n",
    "dataset = dataset[:int(limit)]\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "\n",
    "# Load an image\n",
    "\n",
    "image_path = dataset[0][0]\n",
    "label = dataset[0][1]\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Dimensions of the image\n",
    "print(\"Width and height respectively: {} Pixels\" .format(image.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first image in the dataset\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(\"First Image Sample\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the data by class\n",
    "\n",
    "BENIGNdata = [img for img, label in dataset if label == 0]\n",
    "MALIGNANTdata = [img for img, label in dataset if label == 1]\n",
    "\n",
    "BENIGNlabels = [label for img, label in dataset if label == 0]\n",
    "MALIGNANTlabels = [label for img, label in dataset if label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of images each type of dataset\n",
    "\n",
    "BENIGNsample = random.sample(BENIGNdata, 50)\n",
    "MALIGNANTsample = random.sample(MALIGNANTdata, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(5, 10, figsize=(20, 10))\n",
    "\n",
    "# Loop through the subplots\n",
    "for n in range(5):\n",
    "    for m in range(10):\n",
    "        # Calculate the index based on row and column\n",
    "        idx = m + 10 * n\n",
    "\n",
    "        if idx < len(BENIGNsample):\n",
    "            # Open the image using PIL\n",
    "            image = Image.open(BENIGNsample[idx])\n",
    "\n",
    "            # Display the image on the current subplot\n",
    "            ax[n, m].imshow(image)\n",
    "            ax[n, m].grid(False)\n",
    "        else:\n",
    "            # If there are not enough images to fill the grid, remove the empty subplot\n",
    "            fig.delaxes(ax[n, m])\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the grid of healthy patches\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(5, 10, figsize=(20, 10))\n",
    "\n",
    "# Loop through the subplots\n",
    "for n in range(5):\n",
    "    for m in range(10):\n",
    "        # Calculate the index based on row and column\n",
    "        idx = m + 10 * n\n",
    "\n",
    "        if idx < len(MALIGNANTsample):\n",
    "            # Open the image using PIL\n",
    "            image = Image.open(MALIGNANTsample[idx])\n",
    "\n",
    "            # Display the image on the current subplot\n",
    "            ax[n, m].imshow(image)\n",
    "            ax[n, m].grid(False)\n",
    "        else:\n",
    "            # If there are not enough images to fill the grid, remove the empty subplot\n",
    "            fig.delaxes(ax[n, m])\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the grid of healthy patches\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "\n",
    "\n",
    "labels = [\"BENIGN\", \"MALIGNANT\"]\n",
    "counts = [len(BENIGNdata), len(MALIGNANTdata)]\n",
    "colors = [\"green\", \"orange\"]\n",
    "\n",
    "total_samples = sum(counts)\n",
    "percentages = [(count / total_samples) * 100 for count in counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, counts, color=colors)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing the samples\n",
    "\n",
    "target_size = (50, 50)\n",
    "resizedBENIGN = []\n",
    "resizedMALIGNANT = []\n",
    "\n",
    "for image_path in BENIGNdata:\n",
    "    image = Image.open(image_path)\n",
    "    new_image = image.resize(target_size, Image.LANCZOS)  # Resize with anti-aliasing for better quality\n",
    "    resizedBENIGN.append(new_image)\n",
    "\n",
    "for image_path in MALIGNANTdata:\n",
    "    image = Image.open(image_path)\n",
    "    new_image = image.resize(target_size, Image.LANCZOS)  # Resize with anti-aliasing for better quality\n",
    "    resizedMALIGNANT.append(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset pixel values\n",
    "\n",
    "BENIGNdataset = np.array([np.array(image) / 255.0 for image in resizedBENIGN])\n",
    "MALIGNANTdataset = np.array([np.array(image) / 255.0 for image in resizedMALIGNANT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "\n",
    "BENIGNdataset = shuffle(BENIGNdataset, random_state=42)\n",
    "MALIGNANTdataset = shuffle(MALIGNANTdataset, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "\n",
    "print(\"BENIGNdataset shape: {}\" .format(BENIGNdataset.shape))\n",
    "print(\"MALIGNANTdataset shape: {}\" .format(MALIGNANTdataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "# Split into training and testing\n",
    "\n",
    "BENIGNtrain, BENIGNtemp, BENIGNtrain_labels, BENIGNtemp_labels = train_test_split(\n",
    "    BENIGNdataset, BENIGNlabels, test_size= 0.2, stratify= BENIGNlabels, random_state= 42\n",
    ")\n",
    "\n",
    "MALIGNANTtrain, MALIGNANTtemp, MALIGNANTtrain_labels, MALIGNANTtemp_labels = train_test_split(\n",
    "    MALIGNANTdataset, MALIGNANTlabels, test_size= 0.2, stratify= MALIGNANTlabels, random_state= 42\n",
    ")\n",
    "\n",
    "# Split validation and test data\n",
    "\n",
    "BENIGNval, BENIGNtest, BENIGNval_labels, BENIGNtest_labels = train_test_split(\n",
    "    BENIGNtemp, BENIGNtemp_labels, test_size= 0.5, stratify= BENIGNtemp_labels, random_state= 42\n",
    ")\n",
    "\n",
    "MALIGNANTval, MALIGNANTtest, MALIGNANTval_labels, MALIGNANTtest_labels = train_test_split(\n",
    "    MALIGNANTtemp, MALIGNANTtemp_labels, test_size= 0.5, stratify= MALIGNANTtemp_labels, random_state= 42\n",
    ")\n",
    "\n",
    "\n",
    "# Combine the couples into a one\n",
    "\n",
    "train_data = np.concatenate((BENIGNtrain, MALIGNANTtrain), axis= 0)\n",
    "train_labels = np.concatenate((BENIGNtrain_labels, MALIGNANTtrain_labels), axis= 0)\n",
    "val_data = np.concatenate((BENIGNval, MALIGNANTval), axis= 0)\n",
    "val_labels = np.concatenate((BENIGNval_labels, MALIGNANTval_labels), axis= 0)\n",
    "test_data = np.concatenate((BENIGNtest, MALIGNANTtest), axis= 0)\n",
    "test_labels = np.concatenate((BENIGNtest_labels, MALIGNANTtest_labels), axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the shape for the labels\n",
    "\n",
    "train_labels = to_categorical(train_labels, 2)\n",
    "val_labels = to_categorical(val_labels, 2)\n",
    "test_labels = to_categorical(test_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_data shape : {}' .format(train_data.shape))\n",
    "print('train_labels shape : {}' .format(train_labels.shape))\n",
    "print('val_data shape : {}' .format(val_data.shape))\n",
    "print('val_labels shape : {}' .format(val_labels.shape))\n",
    "print('test_data shape : {}' .format(test_data.shape))\n",
    "print('test_labels shape : {}' .format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Convolutional Layers\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(50, 50, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling layer after the first Conv2D\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((3, 3), strides=2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((3, 3), strides=2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((3, 3), strides=2),\n",
    "\n",
    "    # Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "\n",
    "model.compile(optimizer= opt, loss= \"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor for early stopping\n",
    "    patience=5,           # Number of epochs with no improvement after which training will stop\n",
    "    restore_best_weights=True  # Restore the model weights from the epoch with the best validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, validation_data= (val_data, val_labels), epochs= 25, batch_size= 148, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ydata = model.predict(test_data)\n",
    "Ylabels = np.argmax(Ydata, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlabels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Matrices\n",
    "\n",
    "accuracy = accuracy_score(Xlabels, Ylabels)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "\n",
    "precision = precision_score(Xlabels, Ylabels)\n",
    "print(f'Precision: {precision:.2f}')\n",
    "\n",
    "recall = recall_score(Xlabels, Ylabels)\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "f1 = f1_score(Xlabels, Ylabels)\n",
    "print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "confusion_mat = confusion_matrix(Xlabels, Ylabels)\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mat, annot=True, linewidths=0.01,cmap=\"BuPu\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
